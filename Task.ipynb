{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82337133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0020b",
   "metadata": {},
   "source": [
    "# Step 1 : Import neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4152f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#from constants import openai_key\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d94b27",
   "metadata": {},
   "source": [
    "# Step 2: Start local LLM server , and point client to base url of the existing server "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7013cbe0",
   "metadata": {},
   "source": [
    "# Step 3:Define roles to the Conversational system for what purpose one is going to utilize , in a way tweaking the system prompt content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07382db",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.chat.completions.create(\n",
    "  model=\"local-model\", # this field is currently unused\n",
    "  messages=[\n",
    "    #{\"role\": \"system\", \"content\": \"Do the name entity tagging in this format Product: [PRODUCT] Time Period: [TIME_PERIOD] Month: [MONTH] Data: [DATA] Chart Type: [CHART_TYPE]\"},\n",
    "    {\"role\": \"system\", \"content\": \"do the name entity tagging  (Product: [PRODUCT] Time Period: [TIME_PERIOD] Month: [MONTH] Data: [DATA] Chart Type: [CHART_TYPE])\"},\n",
    "    {\"role\": \"user\", \"content\": user_input}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88693e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cbf6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.choices[0].message)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8280c2d9",
   "metadata": {},
   "source": [
    "    Product: Samsung\n",
    "    Time Period: November 2022 and December 2022\n",
    "    Month: November and December\n",
    "    Data: Sale data for Samsung products during the specified time period\n",
    "    Chart Type: Bar chart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef5767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "612a251c",
   "metadata": {},
   "source": [
    "# user can also play with its parameter to set the output query response like shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI GPT-3 API key\n",
    "openai = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "\n",
    "# Example user query\n",
    "user_query = \"Show me last 4 months sale of product iphone as bar chart.\"\n",
    "\n",
    "# Structure the prompt to get named entity tagging format\n",
    "prompt = f\"User Query: {user_query}\\nResponse: Product: [PRODUCT] Time Period: [TIME_PERIOD] Month: [MONTH] Data: [DATA] Chart Type: [CHART_TYPE]\"\n",
    "\n",
    "# Generate response\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",  # You can experiment with different engines\n",
    "    prompt=prompt,\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Extract information from the generated response\n",
    "extracted_info = response['choices'][0]['text'].strip()\n",
    "\n",
    "# Print the extracted information\n",
    "print(extracted_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a44aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b69200a",
   "metadata": {},
   "source": [
    "# Now let's create a chains of prompts which take input and returns output in a chain of different scenario's !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49172a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740c92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Integrate our code OpenAI API\n",
    "import os\n",
    "from constants import openai_key\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=openai_key\n",
    "\n",
    "# streamlit framework\n",
    "\n",
    "st.title('STmicroelectronics')\n",
    "input_text=st.text_input(\"Search whatever you want ?\")\n",
    "\n",
    "# Prompt Templates\n",
    "\n",
    "first_input_prompt=PromptTemplate(\n",
    "    input_variables=['name'],\n",
    "    template=\"is {name} a good person?\"\n",
    ")\n",
    "\n",
    "# Memory\n",
    "\n",
    "person_memory = ConversationBufferMemory(input_key='name', memory_key='chat_history')\n",
    "dob_memory = ConversationBufferMemory(input_key='person', memory_key='chat_history')\n",
    "descr_memory = ConversationBufferMemory(input_key='dob', memory_key='description_history')\n",
    "\n",
    "## OPENAI LLMS\n",
    "llm=OpenAI(temperature=0.8)\n",
    "chain=LLMChain(\n",
    "    llm=llm,prompt=first_input_prompt,verbose=True,output_key='person',memory=person_memory)\n",
    "\n",
    "# Prompt Templates\n",
    "\n",
    "second_input_prompt=PromptTemplate(\n",
    "    input_variables=['person'],\n",
    "    template=\"tell about whether this {person} \"\n",
    ")\n",
    "\n",
    "chain2=LLMChain(\n",
    "    llm=llm,prompt=second_input_prompt,verbose=True,output_key='dob',memory=dob_memory)\n",
    "# Prompt Templates\n",
    "\n",
    "third_input_prompt=PromptTemplate(\n",
    "    input_variables=['dob'],\n",
    "    template=\"Mention 5 major events happened around {dob} in the world\"\n",
    ")\n",
    "chain3=LLMChain(llm=llm,prompt=third_input_prompt,verbose=True,output_key='description',memory=descr_memory)\n",
    "parent_chain=SequentialChain(\n",
    "    chains=[chain,chain2,chain3],input_variables=['name'],output_variables=['person','dob','description'],verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "if input_text:\n",
    "    st.write(parent_chain({'name':input_text}))\n",
    "\n",
    "    with st.expander('Person Name'): \n",
    "        st.info(person_memory.buffer)\n",
    "\n",
    "    with st.expander('Major Events'): \n",
    "        st.info(descr_memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
